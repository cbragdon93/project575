---
title: "Preliminary_Analysis"
author: "Callen"
date: "9/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries
```{r}
library(ggplot2)
library(multcomp)
library(tidyverse)
library(broom)
library(tidyverse)
library(gridExtra)
# The package I normally use for time series, TSA, got discontinued
# from the CRAN repo. Rather than dealing with archives
# and possible versioning issues, I'm currently using a set of libraries that
# do the same thing
library(fable)
library(forecast)
```

## Core Functions
```{r}
# = signs are usually used to keep an object's scope to only be in the 
# function/function call.
# e.g. if I do mean(x=1:10), x won't exist, but if I do mean(x<-1:10),
# x will exist
extendLimit<-function(v, whichAxis, min=NULL){
  myMax=max(v)
  switch(tolower(whichAxis),
    "x"=expand_limits(x=c(min,myMax+15)),
    "y"=expand_limits(y=c(min,myMax+15))
  )
}
```


## Bike
```{r}
# Read in files
bikeByDay <- read.csv("./data/bike/day.csv")
bikeByDayAndHour <- read.csv("./data/bike/hour.csv")
```

### Basic Plots
```{r}
# just a time plot, regardless of other factors
DayDate<-as.Date(bikeByDay$dteday)
bikeByDayPlot<-ggplot(bikeByDay, aes(x=dteday, y=cnt)) +
  geom_point() + theme_bw() + labs(title="Total Rentals By Day", y="Total", x="Day") + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())
bikeByDayPlot
```
```{r}
# just a time plot, marked by if it's a work day
# need to change the type of the workingday column, integer
# (meant as boolean) into factor
bikeByDay$workingday<-as.factor(bikeByDay$workingday)
ggplot(bikeByDay, aes(x=dteday, y=cnt, col=workingday)) +
  geom_point() +
  theme_bw() +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
  labs(title="Total Bike Rentals, marked by working day", 
       x="Date", y="Total", col="Working Day?")
```

```{r}
# Seasonal rental
# could isolate the seasons and model those if you want? 
# But seasonal structure wouldn't be bad to do forecasting on
ggplot(bikeByDay, aes(x=dteday, y=cnt, col=season)) +
  geom_point() +
  theme_bw() +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
  labs(title="Total Bike Rentals, marked by season", 
       x="Date", y="Total", col="Season")
```
The main purpose of these graphs is to show the ways you can group your data 
to plot in a relatively straightforward way.


Now, there are time series-related models that I think will be discussed in class
later on. We could use that information to not only create a stronger model, but
to more easily predict future total usage, given the seasonal structure of
the above plot.


## Forest
```{r}
forestData<-read.csv("data/forestFire/forestfires.csv")
```


## Facebook

```{r}
facebook<-read.csv("data/facebook/dataset_Facebook.csv", sep=";")
```


Here, I'll go  through code that implements a simplistic (Generalized) Linear model. The model
will claim that, on each separate day, there's a linear dependence of likes on the time
of day you post. 

R users have a strong aversion to "for" loops, and use "apply" functions instead.
"For" loops are slower in R, with applies being faster, and matrix operations
(i.e. vectorized code) being the fastest. 
```{r}
colnames(facebook)[which(colnames(facebook)=="Post.Hour")]<-"PostHour"
colnames(facebook)[which(colnames(facebook)=="Post.Weekday")]<-"PostWeekday"
lapply(unique(facebook$PostWeekday), function(x){
  # building simple linear models with R.
  # Something we could ask of the facebookk data could be
  # "When is the best day to post? Best Hour? Best Day and hour?"
  subsetData = facebook[which(facebook$PostWeekday==x),]
  theseLikes = subsetData$like
  isPaid = subsetData$Paid
  theseHours = subsetData$PostHour
  # extracting slopes, intercepts of model
  # both for paid and unpaid
  m1 = glm(theseLikes ~ as.factor(isPaid) + theseHours, data=subsetData)
  print(m1)
  thisIntercept = m1$coefficients[[1]]
  paidCoef = m1$coefficients[[2]]
  h = m1$coefficients[[3]]
  ggplot(subsetData, aes(x=PostHour, y=like, col=Paid)) +
  geom_point() +
  # fit the model
  geom_abline(slope = h, intercept=(thisIntercept+paidCoef), color="blue") +
    geom_abline(slope=h, intercept=thisIntercept, color="black")+
  theme_bw() +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
  labs(title=paste0("Facebook Like Activity: PostDay #: ",x), 
       x="post hour", y="likes", col="paid?")
})
```

These models aren't necessarily good by any means, but are meant as a way to show how one can do this
sort of thing in R, moreso with an automated mindset. Things we'd want to inspect
to evaluate a model's performance is, for example, the model's AIC. AIC is
used as a metric to compare two model; lower AIC relative to another's is the
goal. However, AIC is used as a relative metric, so if you had a bunch of bad
models, picking the lowest AIC isn't a surefire way standalone, but there are
other methods that will probably be mentioned later in class.

